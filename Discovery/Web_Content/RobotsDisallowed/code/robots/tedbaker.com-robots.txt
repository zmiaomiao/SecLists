

# For all robots
User-agent: *

# Block access to specific groups of pages


Disallow: /de/cart
Disallow: /de/checkout
Disallow: /de/my-account
Disallow: /de/search?*

Disallow: /fr/cart
Disallow: /fr/checkout
Disallow: /fr/my-account
Disallow: /fr/search?*

Disallow: /ie/cart
Disallow: /ie/checkout
Disallow: /ie/my-account
Disallow: /ie/search?*

Disallow: /neu/cart
Disallow: /neu/checkout
Disallow: /neu/my-account
Disallow: /neu/search?*

Disallow: /nl/cart
Disallow: /nl/checkout
Disallow: /nl/my-account
Disallow: /nl/search?*

Disallow: /oeu/cart
Disallow: /oeu/checkout
Disallow: /oeu/my-account
Disallow: /oeu/search?*

Disallow: /row/cart
Disallow: /row/checkout
Disallow: /row/my-account
Disallow: /row/search?*

Disallow: /seu/cart
Disallow: /seu/checkout
Disallow: /seu/my-account
Disallow: /seu/search?*

Disallow: /uk/cart
Disallow: /uk/checkout
Disallow: /uk/my-account
Disallow: /uk/search?*


# Block access to disallowed sites


Disallow: /oeu
Disallow: /seu
Disallow: /neu

Request-rate: 1/10              # maximum rate is one page every 10 seconds
Crawl-delay: 10                 # 10 seconds between page requests
Visit-time: 0400-0845           # only visit between 04:00 and 08:45 UTC

# Allow search crawlers to discover the sitemap
Sitemap: /sitemap.xml

# Block CazoodleBot as it does not present correct accept content headers
User-agent: CazoodleBot
Disallow: /

# Block MJ12bot as it is just noise
User-agent: MJ12bot
Disallow: /

# Block dotbot as it cannot parse base urls properly
User-agent: dotbot/1.0
Disallow: /

# Block Gigabot
User-agent: Gigabot
Disallow: /
