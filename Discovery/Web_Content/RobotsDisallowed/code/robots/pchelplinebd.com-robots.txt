# 
# This file contains rules to prevent the crawling and indexing of certain parts 
# of your web site by spiders of a major search engines likes Google and Yahoo. 
# By managing these rules you can allow or disallow access to specific folders
# and files for such spyders. 
# The good way to hide private data or save a lot of bandwidth. 
#
#
# For more information about the robots.txt standard, see:
# http://www.robotstxt.org/wc/robots.html
#
# For syntax checking, see:
# http://www.sxw.org.uk/computing/robots/check.html


User-agent: *

#Files
Disallow: ow_version.xml
Disallow: INSTALL.txt
Disallow: LICENSE.txt
Disallow: README.txt
Disallow: UPDATE.txt
Disallow: CHANGES.txt

# URLs
Disallow: /admin/

    Sitemap: http://www.pchelplinebd.com/sitemap.xml
#    User-agent: Googlebot
#    Disallow: /*/trackback
#    Disallow: /*/feed
#    Disallow: /*/comments
#    Disallow: /*?*
#    Disallow: /*?
#    Disallow: /*page/*

    User-agent: *
    Disallow: /cgi-bin/
    Disallow: /wp-admin/
    Disallow: /wp-includes/
    Disallow: /wp-content/plugins/
    Disallow: /wp-content/themes/
    Disallow: /trackback
    Disallow: /comments
    Disallow: /feed
    
#User-agent: Googlebot
#Allow: /wp-content/plugins/wptouch-pro-3/
#User-agent: Googlebot
#Allow: /wp-content/wptouch-data/

Sitemap: http://www.pchelplinebd.com/sitemap.xml

User-agent: Googlebot
Disallow: 
