# Robots.txt file created by http://www.webtoolcentral.com
# For domain: http://www.bennadel.com

# Point all spiders to my Google sitemap.
Sitemap: http://www.bennadel.com/index.cfm?event=google.sitemap

# All robots will spider the domain
User-agent: *
Disallow:

# The number of seconds search engines should delay between spidering requests.
User-agent: *
Crawl-delay: 10

# Disallow directory
User-agent: *
Disallow: /admin/
Disallow: /linked/
Disallow: /resources/skin_spider/application/
Disallow: /resources/team_nylon/
Disallow: /resources/demo/kinky_calendar/
Disallow: /resources/projects/kinky_calendar/demo/
Disallow: /huge-ass/go/
Disallow: /shorturl/
Disallow: /short-url/

