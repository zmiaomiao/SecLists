# See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file
#
# To ban all spiders from the entire site uncomment the next two lines:
# User-agent: *
# Disallow: /

# http://memorva.jp/memo/mobile/allow_crawler_agent_ip.php
# http://d.hatena.ne.jp/atyks/20081222/1229945928
User-Agent: Googlebot-Mobile
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: Y!J-MBS
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: Y!J-SRD
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: LD_mobile_bot
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: ichiro/mobile
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: moba-crawler
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: froute.jp
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

User-agent: RFCrawler-Mobile
Sitemap: http://byoinnavi.jp/sitemaps/mobile_sitemap.xml.gz

# http://www.sitemaps.org/ja/protocol.php
User-Agent: *
Sitemap: http://byoinnavi.jp/sitemaps/sitemap.xml.gz
Disallow: /admin
Disallow: /api
Disallow: /clinic/*/z*
Disallow: /clinic/z
Disallow: /info/na_
Disallow: /info/not_acceptable
Disallow: /job
Disallow: /map
Disallow: /member
Disallow: /parts
Disallow: /query
Disallow: /service/bsa
Disallow: /test

User-agent: Mediapartners-Google
Disallow:
Disallow: /parts
Disallow: /member
Disallow: /admin
