# This robots.txt file requests that search engines and other
# automated web-agents don't try to index the files in this
# directory. This file is required in the event that you use
# Revive Adserver without virtual domains (i.e. you use a
# single URL to run both the admin interface and the delivery
# engine), and have the web root set to this directory.
User-agent: *
Disallow: /images/
Disallow: /xc/
Disallow: /files/
Disallow: /tv/
Disallow: /TV_JTV/
Disallow: /js/
Disallow: /ggx/
Disallow: /commfort_stat/
Disallow: /css/
Disallow: /webinc/

Crawl-delay: 10