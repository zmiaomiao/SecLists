## GENERAL SETTINGS

## Enable robots.txt rules for all crawlers
User-agent: *

## Crawl-delay parameter: number of seconds to wait between successive requests to the same server.
## Set a custom crawl rate if you're experiencing traffic problems with your server.
Crawl-delay: 60


User-agent: *
Disallow: /admin
Disallow: /copy
Disallow: /cart-fancybox
Disallow: /copyb
Disallow: /backup

Sitemap: http://www.decorsteals.com/sitemap.xml
